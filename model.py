# -*- coding: utf-8 -*-
"""exp (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAvcChKf_2Z-6dZLniVbomJFmSkTDnNo
"""

import numpy as np
import pandas as pd
from io import StringIO
import torch
import os

import py_midicsv as pm #https://github.com/timwedde/py_midicsv
from midi_player import MIDIPlayer #https://pypi.org/project/midi-player/

def play_midi_file(path):
  return MIDIPlayer(path,200)

def df_to_pymidi(df):

    df = df.copy()

    df["Start"] = df["Start"].cumsum()
    df["Event"] = " Note_on_c"
    df["Track"] = 1
    df["Channel"] = 0
    df["Order"]=1

    df_stop = df.copy()
    df_stop["Event"] = " Note_off_c"
    df_stop["Start"] = df_stop["Start"] +  df_stop["Duration"]
    df_stop["Order"]=0

    df = pd.concat([df,df_stop],axis=0)
    df = df.reset_index()
    df = df.sort_values(["Start","Order"],axis=0,ascending=[True, True])

    df = df[["Track","Start","Event","Channel","Note","Volume"]]
    df = df.rename(columns={"Start":"Time"})

    header = ['0, 0, Header, 0, 1, 384\n',
            '1, 0, Start_track\n',
            '1, 0, Time_signature, 4, 2, 24, 8\n', #Track, Time, Time_signature, Num, Denom, Ticks per beat, NotesQ
            '1, 0, Tempo, 645161\n', #Number of microseconds per quarter note
            '1, 0, Title_t, "Elec. Piano (Classic)"\n',
            '1, 0, Program_c, 0, 0\n']

    csv_buffer = StringIO()
    df.to_csv(csv_buffer,header=False,index=False)
    event_list = csv_buffer.getvalue().split("\n")
    if(event_list[-1]==""):
        event_list = event_list[:-1]
    event_list = [x.replace(".0", "")+"\n" for x in event_list]

    maxT = df["Time"].iloc[-1]
    end_track = [f'1, {maxT:d}, End_track\n']

    song = header + event_list + end_track + ['0, 0, End_of_file']

    midi_object = pm.csv_to_midi(song)

    return midi_object

def pymidi_to_midi_file(midi_object, out_file):

    with open(out_file, "wb") as output_file:
        midi_writer = pm.FileWriter(output_file)
        midi_writer.write(midi_object)

def df_to_midi_file(df, out_file):
    midi_object = df_to_pymidi(df)
    pymidi_to_midi_file(midi_object, out_file)

def torch_to_df(X):
    df = pd.DataFrame({"Start":X[:,0].numpy(), "Duration":X[:,1].numpy(),"Note":X[:,2].numpy(),"Volume":X[:,3].numpy()})
    return df

def pt_to_df(file_pt):
    X = torch.load(file_pt)
    df = torch_to_df(X)
    return df

def torch_to_midi_file(X,out_file):
    df = torch_to_df(X)
    df_to_midi_file(df,out_file)

def play_torch(X):
    torch_to_midi_file(X,"temp.mid")
    return(play_midi_file("temp.mid"))

def play_pt_file(file):
    X = torch.load(file)
    return(play_torch(X))

from torch.utils.data import Dataset, DataLoader
import glob
import matplotlib.pyplot as plt

class NextNoteDataset(Dataset):
    def __init__(self, root_dir, context_window=196, overlap=True, stride=2):
        """
        Initialize the dataset by loading all the song data and extracting training instances.
        Args:
            root_dir (str): Path to the root directory containing song data (organized by artist folders).
            context_window (int): Size of the context window (number of previous notes to consider).
            overlap (bool): Whether to extract overlapping instances.
        """
        self.context_window = context_window
        self.overlap = overlap
        self.data = []

        print(f"Loading data from {root_dir}...")  # Debug: Check if the path is being read
        files = glob.glob(f"{root_dir}/**/*.pt", recursive=True)
        if not files:
            print("No .pt files found in the specified directory.")
        else:
            print(f"Found {len(files)} .pt files.")

        # Load all .pt files recursively
        for file in files:
            try:
                print(f"Loading file: {file}")  # Debug: Log each file being processed
                song_data = torch.load(file).float()  # Ensure data is in float32 format
                L = song_data.shape[0]
                print(f"Loaded file: {file}, Number of rows: {L}")

                # Extract instances (context and target)
                if overlap:
                    step = stride
                else:
                    step = context_window + 1
                for i in range(0, L - context_window, step):
                    context = song_data[i:i + context_window]
                    target = song_data[i + context_window]
                    self.data.append((context, target))

            except Exception as e:
                print(f"Error loading {file}: {e}")

        print(f"Total instances loaded: {len(self.data)}")

    def __len__(self):
        """
        Return the total number of instances.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Get the context and target for the given index.
        Args:
            idx (int): Index of the instance.
        Returns:
            tuple: A tuple (context, target), where:
                - context (torch.Tensor): Tensor of shape (context_window, 4).
                - target (torch.Tensor): Tensor of shape (4,).
        """
        context, target = self.data[idx]
        return context, target

data_dir = "./data/songs/train/"
data_loader = NextNoteDataset(data_dir, context_window=196, overlap=True, stride=16)

def gaussian_nll_loss(pred_params, target):
    """
    Compute negative log-likelihood loss for Gaussian distribution
    pred_params: tensor containing [mu, sigma]
    target: actual values
    """
    mu = pred_params[:, 0]
    sigma = pred_params[:, 1]

    # Ensure sigma is positive
    sigma = torch.clamp(sigma, min=1e-6)

    # Calculate NLL for Gaussian distribution using tensor constants
    pi_tensor = torch.tensor(2 * np.pi).to(pred_params.device)
    nll = 0.5 * (torch.log(pi_tensor) + 2 * torch.log(sigma) + ((target - mu) / sigma) ** 2)
    return nll.mean()

import torch.nn as nn
import torch.nn.functional as F
def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")
class MusicRNN(nn.Module):
    def __init__(self, input_size=4, hidden_sizes=(128, 256), output_size=134):
        super(MusicRNN, self).__init__()

        # RNN layers
        self.rnn1 = nn.RNN(input_size, hidden_sizes[0], batch_first=True)
        self.rnn2 = nn.RNN(hidden_sizes[0], hidden_sizes[1], batch_first=True)

        # Output heads
        self.time_head = nn.Sequential(
            nn.Linear(hidden_sizes[1], hidden_sizes[1]//2),
            nn.ReLU(),
            nn.Linear(hidden_sizes[1]//2, 2)  # [μ_t, σ_t]
        )

        self.duration_head = nn.Sequential(
            nn.Linear(hidden_sizes[1], hidden_sizes[1]//2),
            nn.ReLU(),
            nn.Linear(hidden_sizes[1]//2, 2)  # [μ_d, σ_d]
        )

        self.note_head = nn.Sequential(
            nn.Linear(hidden_sizes[1], hidden_sizes[1]),
            nn.ReLU(),
            nn.Linear(hidden_sizes[1], 128)  # logits for 128 notes
        )

        self.volume_head = nn.Sequential(
            nn.Linear(hidden_sizes[1], hidden_sizes[1]//2),
            nn.ReLU(),
            nn.Linear(hidden_sizes[1]//2, 2)  # [μ_v, σ_v]
        )

        # Move model to GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        out, _ = self.rnn1(x)
        out, _ = self.rnn2(out)

        # Take the last time step
        last_hidden = out[:, -1, :]

        # Generate parameters for each component
        time_params = self.time_head(last_hidden)
        duration_params = self.duration_head(last_hidden)
        note_logits = self.note_head(last_hidden)
        volume_params = self.volume_head(last_hidden)

        # Ensure positive standard deviations with softplus
        time_mu, time_sigma = time_params[:, 0], F.softplus(time_params[:, 1])
        dur_mu, dur_sigma = duration_params[:, 0], F.softplus(duration_params[:, 1])
        vol_mu, vol_sigma = volume_params[:, 0], F.softplus(volume_params[:, 1])

        # Combine all outputs
        output = torch.cat([
            time_mu.unsqueeze(1),
            time_sigma.unsqueeze(1),
            dur_mu.unsqueeze(1),
            dur_sigma.unsqueeze(1),
            note_logits,
            vol_mu.unsqueeze(1),
            vol_sigma.unsqueeze(1)
        ], dim=1)

        return output

# Training function
def train_model_rnn(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    train_losses = []
    val_losses = []

    for epoch in tqdm(range(num_epochs)):
        model.train()
        total_train_loss = 0

        for batch_idx, (context, target) in enumerate(train_loader):
            context = context.float().to(device)
            target = target.float().to(device)

            optimizer.zero_grad()
            output = model(context)

            # Split output into components
            time_params = output[:, :2]
            duration_params = output[:, 2:4]
            note_logits = output[:, 4:132]
            volume_params = output[:, 132:]

            # Calculate losses using F.gaussian_nll_loss
            time_loss = F.gaussian_nll_loss(time_params[:, 0], target[:, 0], time_params[:, 1]**2)
            duration_loss = F.gaussian_nll_loss(duration_params[:, 0], target[:, 1], duration_params[:, 1]**2)
            note_loss = F.cross_entropy(note_logits, target[:, 2].long())
            volume_loss = F.gaussian_nll_loss(volume_params[:, 0], target[:, 3], volume_params[:, 1]**2)

            loss = time_loss + duration_loss + note_loss + volume_loss
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

            if batch_idx % 10000 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')

        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # Validation
        model.eval()
        total_val_loss = 0

        with torch.no_grad():
            for context, target in val_loader:
                context = context.float().to(device)
                target = target.float().to(device)

                output = model(context)

                time_params = output[:, :2]
                duration_params = output[:, 2:4]
                note_logits = output[:, 4:132]
                volume_params = output[:, 132:]

                time_loss = F.gaussian_nll_loss(time_params[:, 0], target[:, 0], time_params[:, 1]**2)
                duration_loss = F.gaussian_nll_loss(duration_params[:, 0], target[:, 1], duration_params[:, 1]**2)
                note_loss = F.cross_entropy(note_logits, target[:, 2].long())
                volume_loss = F.gaussian_nll_loss(volume_params[:, 0], target[:, 3], volume_params[:, 1]**2)

                loss = time_loss + duration_loss + note_loss + volume_loss
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f'Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')

    return train_losses, val_losses


train_size = int(0.85 * len(data_loader))
val_size = len(data_loader) - train_size
from torch.utils.data import random_split
train_dataset, val_dataset = random_split(data_loader, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

print(f"Training set size: {len(train_dataset)}")
print(f"Validation set size: {len(val_dataset)}")
#run the model and save it
model_rnn = MusicRNN().to(get_device())
train_losses, val_losses = train_model_rnn(model=model_rnn, train_loader=train_loader, val_loader=val_loader)

# Plot the learning curves
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend()
plt.show()

#save the model
torch.save(model_rnn.state_dict(), 'model_rnn.pt')

def generate_test_predictions_rnn(model, test_file_path):
    # Load and preprocess test data
    test_data = torch.load(test_file_path).float()  # Shape: [794, 196, 4]
    device = get_device()
    model.eval()

    predictions = []

    # Process each sequence of 196 notes in test data
    with torch.no_grad():
        for i in tqdm(range(len(test_data))):
            context = test_data[i].to(device)  # Get current sequence of 196 notes

            # Get model output for current context
            output = model(context.unsqueeze(0))  # Shape: [1, 134]

            # Add full output vector to predictions
            predictions.append(output[0])  # Shape: [134]
    torch.save(torch.stack(predictions), "note_predictions.pt")

    return torch.stack(predictions)  # Shape: [794, 134]

test_file = "./data/songs/test.pt"
predictions = generate_test_predictions_rnn(model_rnn, test_file)

print("\nVerifying prediction format:")
print(f"Shape: {predictions.shape}")
print("\nFirst prediction:")
print("Time params (μ, σ):", predictions[0, :2].cpu().numpy())
print("Duration params (μ, σ):", predictions[0, 2:4].cpu().numpy())
print("Note logits shape:", predictions[0, 4:132].shape)
print("Volume params (μ, σ):", predictions[0, 132:].cpu().numpy())